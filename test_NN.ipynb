{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Dropout, Embedding\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../CS230_Project\"))\n",
    "\n",
    "# when training on AWS p2.xlarge, this command will ensure that you're training with the GPU: \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# you can run this command in a separate terminal tab in JupyterLab to monitor and sanity check whether your training is actually using GPU:\n",
    "# $ watch -n 1 nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('dataset_clean_bow_cpy.csv')  # TODO: ADD FILE NAME HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = df['Lyrics'].values\n",
    "years = df['Year'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train, lyrics_test, y_train, y_test = train_test_split(lyrics, years, test_size = 0.3, random_state = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample \n",
    "\n",
    "print('\\n\\nResampling data to even distributions...')\n",
    "\n",
    "# recombine for downsampling\n",
    "tr_len = lyrics_train.shape[0]\n",
    "lyr_tr = lyrics_train.reshape((tr_len,1))\n",
    "y_tr = y_train.reshape((tr_len,1))\n",
    "tr_set = np.concatenate([lyr_tr,y_tr],axis=1)\n",
    "# tr_set[tr_set[:,1] == 0]   <-- like this \n",
    "\n",
    "# separate into classes\n",
    "class_0 = tr_set[tr_set[:,1]==0]\n",
    "class_1 = tr_set[tr_set[:,1]==1]\n",
    "class_2 = tr_set[tr_set[:,1]==2]\n",
    "class_3 = tr_set[tr_set[:,1]==3]\n",
    "class_4 = tr_set[tr_set[:,1]==4]\n",
    "class_5 = tr_set[tr_set[:,1]==5]\n",
    "\n",
    "# Downsample classes 2, 3, 4 to 10K samples\n",
    "n_DS = 10000\n",
    "class_2_DS = resample(class_2, replace=True, n_samples=n_DS, random_state = 27)\n",
    "class_3_DS = resample(class_3, replace=True, n_samples=n_DS, random_state = 27)\n",
    "class_4_DS = resample(class_4, replace=True, n_samples=n_DS, random_state = 27)\n",
    "\n",
    "# Upsample class 0,1,5 to 10K samples \n",
    "class_1_US = resample(class_1, replace=True, n_samples=10000, random_state = 27)\n",
    "class_0_US = resample(class_0, replace=True, n_samples=10000, random_state = 27)\n",
    "class_5_US = resample(class_5, replace=True, n_samples=10000, random_state = 27)\n",
    "\n",
    "# Recombine resampled datasets \n",
    "tr_set_resamp = np.concatenate([class_0_US, class_1_US, class_2_DS, class_3_DS, class_4_DS, class_5_US],axis=0)\n",
    "\n",
    "lyrics_train_resamp = tr_set_resamp[:,0]\n",
    "y_train_resamp = tr_set_resamp[:,1]\n",
    "\n",
    "print('\\nData done resampling\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.countplot(y_train_resamp) # resampled data, should be more evenly distributed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(lyrics_train_resamp)\n",
    "\n",
    "X_train = vectorizer.transform(lyrics_train_resamp)\n",
    "X_test = vectorizer.transform(lyrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes is 6 if grouping by decade, 11 if grouping by 5 years\n",
    "n_classes = 6\n",
    "\n",
    "years_train = tf.keras.utils.to_categorical(y_train_resamp,num_classes=n_classes)\n",
    "years_test = tf.keras.utils.to_categorical(y_test,num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(n_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "my_batch_size = 25\n",
    "n_epochs = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, years_train, \n",
    "                    epochs=n_epochs,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, years_test),\n",
    "                    batch_size=my_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_NN_bigger_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, years_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy)) \n",
    "loss, accuracy = model.evaluate(X_test, years_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
